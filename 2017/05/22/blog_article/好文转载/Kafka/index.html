<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.cat.net/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|Roboto Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
  <link rel="stylesheet" href="/lib/pace/pace-theme-mac-osx.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"dcbupt.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Kafka知识">
<meta property="og:type" content="article">
<meta property="og:title" content="Kafka">
<meta property="og:url" content="http://dcbupt.github.io/2017/05/22/blog_article/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/Kafka/index.html">
<meta property="og:site_name" content="dcddc">
<meta property="og:description" content="Kafka知识">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://kafka.apache.org/0102/images/kafka-apis.png">
<meta property="og:image" content="http://img.blog.csdn.net/20160618151524374?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="http://kafka.apache.org/0102/images/log_anatomy.png">
<meta property="og:image" content="http://cdn1.infoqstatic.com/statics_s1_20170516-0317u5/resource/articles/kafka-analysis-part-1/zh/resources/0310021.png">
<meta property="og:image" content="http://cdn1.infoqstatic.com/statics_s1_20170516-0317u5/resource/articles/kafka-analysis-part-1/zh/resources/0310022.png">
<meta property="og:image" content="http://cdn1.infoqstatic.com/statics_s2_20170530-0600u1/resource/articles/kafka-analysis-part-1/zh/resources/0310024.png">
<meta property="og:image" content="http://kafka.apache.org/0102/images/consumer-groups.png">
<meta property="og:image" content="http://cdn1.infoqstatic.com/statics_s1_20170516-0317u5/resource/articles/kafka-analysis-part-1/zh/resources/0310025.png">
<meta property="og:image" content="http://cdn1.infoqstatic.com/statics_s1_20170516-0317u5/resource/articles/kafka-analysis-part-1/zh/resources/0310026.png">
<meta property="article:published_time" content="2017-05-22T02:55:00.000Z">
<meta property="article:modified_time" content="2022-05-01T06:44:18.726Z">
<meta property="article:author" content="西米大人">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://kafka.apache.org/0102/images/kafka-apis.png">

<link rel="canonical" href="http://dcbupt.github.io/2017/05/22/blog_article/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/Kafka/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
  <title>Kafka | dcddc</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">dcddc</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">西米大人的博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://dcbupt.github.io/2017/05/22/blog_article/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/Kafka/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/me.png">
      <meta itemprop="name" content="西米大人">
      <meta itemprop="description" content="<blockquote class="blockquote-center">优秀的人，不是不合群，而是他们合群的人里面没有你</blockquote>">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="dcddc">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Kafka
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-05-22 10:55:00" itemprop="dateCreated datePublished" datetime="2017-05-22T10:55:00+08:00">2017-05-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-05-01 14:44:18" itemprop="dateModified" datetime="2022-05-01T14:44:18+08:00">2022-05-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/" itemprop="url" rel="index"><span itemprop="name">好文转载</span></a>
                </span>
            </span>

          
            <span id="/2017/05/22/blog_article/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/Kafka/" class="post-meta-item leancloud_visitors" data-flag-title="Kafka" title="热度">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">热度:</span>
              <span class="leancloud-visitors-count"></span>℃
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">评论:</span>
    
    <a title="valine" href="/2017/05/22/blog_article/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/Kafka/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2017/05/22/blog_article/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/Kafka/" itemprop="commentCount"></span>℃
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">字数：</span>
              <span>13k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>22 分钟</span>
            </span>
            <div class="post-description"><blockquote class="blockquote-center">Kafka知识</blockquote></div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h1><p>摘自Kafka官网</p>
<p><code>concepts</code>：</p>
<ul>
<li>Kafka is run as a cluster on one or more servers.</li>
<li>The Kafka cluster stores streams of records in categories called topics.</li>
<li>Each record consists of a key, a value, and a timestamp.</li>
</ul>
<p><code>Four core APIs</code>：</p>
<ul>
<li>The Producer API allows an application to publish a stream of records to one or more Kafka topics.</li>
<li>The Consumer API allows an application to subscribe to one or more topics and process the stream of records produced to them.</li>
<li>The Streams API allows an application to act as a stream processor, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.</li>
<li>The Connector API allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table.<br><img src="http://kafka.apache.org/0102/images/kafka-apis.png"></li>
</ul>
<p><code>Kafka消息流程</code><br>Assigning the partitions in the topic to the consumers in the consumer group so that each partition is consumed by exactly one consumer in the group. </p>
<p>By doing this we ensure that the consumer is the only reader of that partition and consumes the data in order. Since there are many partitions this still balances the load over many consumer instances. Note however that there cannot be more consumer instances in a consumer group than partitions.</p>
<blockquote>
<p>这种消息分发方式，保证了并行高效率消费消息的同时，每个Consumer仍然能按序消费消息</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20160618151524374?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center"></p>
<p>（1）消息生产者Producer将消息发送到kafka broker 的Topic中（每个topic中有多个消息）<br>（2）每个topic中消息增多，topic中的消息进行分区，使得consumer可快速定位消费消息，也可同时消费多个分区中的数据，提高了传输速率。<br>（3）Consumer Group：消费者组里有多个消费者，每个消费者对应着不同partition分区中的数据处理。例如partition1的数据给Consumer 1，2给2，使得多个分区组成的topic由group中的多个consumer消费，也就是数据内部对相同message的并发处理。</p>
<h2 id="topic-amp-partition"><a href="#topic-amp-partition" class="headerlink" title="topic &amp; partition"></a>topic &amp; partition</h2><p><code>topic及分区</code><br>一个topic是对一组消息的归纳。对每个topic，Kafka 对它的日志进行了分区。每个分区都由一系列有序的、不可变的消息组成，这些消息被连续的追加到分区中。<br>分区的意义：consumer可快速定位消费消息，也可同时消费多个分区中的数据，提高了传输速率</p>
<p><code>Partition</code><br>分区的目的：<br>（1）减缓日志文件占用磁盘空间<br>大量消息，broker都要持久化到文件中，硬盘占用空间增大。分区将消息颗粒细分，<code>每个partition可以存放在不同的硬盘空间中</code>，避免单个broker中的topic文件侵占内存空间<br>（2）不同的consumer同时处理partition中的数据<br><code>Kafka中consumer与partition为 1：n的关系，即1个分区仅由1个消费者消费；1个消费者可以同时消费多个不同分区</code>。分区细化消息粒度，消费者同时处理多个分区的越多，达到高效的消息并发处理。</p>
<p><code>topics &amp; partitioned log</code><br>A topic is a category or feed name to which records are published. Topics in Kafka are always multi-subscriber; that is, a topic can have zero, one, or many consumers that subscribe to the data written to it.</p>
<p>For each topic, the Kafka cluster maintains a partitioned log that looks like this:<br><img src="http://kafka.apache.org/0102/images/log_anatomy.png"></p>
<blockquote>
<p>partitioned log维护topic的所有partition的offset信息</p>
</blockquote>
<p><code>Partition的物理存储——文件夹</code><br>为了使得Kafka的吞吐率可以线性提高，物理上把Topic分成一个或多个Partition，<code>每个Partition在物理上对应一个文件夹，该文件夹下存储这个Partition的所有消息和索引文件。</code></p>
<p>若创建topic1和topic2两个topic，且分别有13个和19个分区，则整个集群上会相应会生成共32个文件夹（本文所用集群共8个节点，此处topic1和topic2 replication-factor均为1），如下图所示。<br><img src="http://cdn1.infoqstatic.com/statics_s1_20170516-0317u5/resource/articles/kafka-analysis-part-1/zh/resources/0310021.png"></p>
<p><code>offset</code><br>分区中的每个消息都有一个连续的序列号叫做offset,用来在分区中唯一的标识这个消息。</p>
<p>The records in the partitions are each assigned a sequential id number called the offset that uniquely identifies each record within the partition</p>
<p><code>record的物理存储 &amp; segment list &amp; segment &amp; log entrie</code><br>一个partition文件夹下有多个日志文件（segment），每个日志文件都是一个log entrie序列，每个log entrie包含一个4字节整型数值（值为N+5），1个字节的”magic value”，4个字节的CRC校验码，其后跟N个字节的消息体。<br>每条消息都有一个当前Partition下唯一的64字节的offset，它指明了这条消息的起始位置。磁盘上存储的消息格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">message length ： 4 bytes (value: 1+4+n)</span><br><span class="line">&quot;magic&quot; value ： 1 byte </span><br><span class="line">crc ： 4 bytes </span><br><span class="line">payload ： n bytes </span><br></pre></td></tr></table></figure>

<p>这个log entries并非由一个文件构成，而是分成多个segment，每个segment以该segment第一条消息的offset命名并以“.kafka”为后缀。另外会有一个索引文件，它标明了每个segment下包含的log entry的offset范围，如下图所示。<br><img src="http://cdn1.infoqstatic.com/statics_s1_20170516-0317u5/resource/articles/kafka-analysis-part-1/zh/resources/0310022.png"></p>
<p>因为每条消息都被append到该Partition中，属于顺序写磁盘，因此效率非常高（经验证，顺序写磁盘效率比随机写内存还要高，这是Kafka高吞吐率的一个很重要的保证）。</p>
<h2 id="producer"><a href="#producer" class="headerlink" title="producer"></a>producer</h2><p><code>Producers</code><br>Producers publish data to the topics of their choice. The producer is responsible for choosing which record to assign to which partition within the topic. </p>
<blockquote>
<p>Producer将消息发布到它指定的topic中,并负责决定发布到哪个分区。通常简单的由负载均衡机制随机选择分区，但也可以通过特定的分区函数选择分区。使用的更多的是第二种。</p>
</blockquote>
<p><code>Producer消息路由</code>：<br>Producer发送消息到broker时，会根据Paritition机制选择将其存储到哪一个Partition。如果Partition机制设置合理，所有消息可以均匀分布到不同的Partition里，这样就实现了负载均衡。</p>
<p>如果一个Topic对应一个文件，那这个文件所在的机器I&#x2F;O将会成为这个Topic的性能瓶颈，而有了Partition后，不同的消息可以并行写入不同broker的不同Partition里，极大的提高了吞吐率。</p>
<p>可以在$KAFKA_HOME&#x2F;config&#x2F;server.properties中通过配置项num.partitions来指定新建Topic的默认Partition数量，也可在创建Topic时通过参数指定，同时也可以在Topic创建之后通过Kafka提供的工具修改。</p>
<p><code>在发送一条消息时，可以指定这条消息的key，Producer根据这个key和Partition机制来判断应该将这条消息发送到哪个Parition。</code></p>
<p>Paritition机制可以通过指定Producer的paritition. class这一参数来指定，该class必须实现kafka.producer.Partitioner接口。</p>
<p>本例中如果key可以被解析为整数则将对应的整数与Partition总数取余，该消息会被发送到该数对应的Partition。（每个Parition都会有个序号,序号从0开始）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import kafka.producer.Partitioner;</span><br><span class="line">import kafka.utils.VerifiableProperties;</span><br><span class="line"></span><br><span class="line">public class JasonPartitioner&lt;T&gt; implements Partitioner &#123;</span><br><span class="line"></span><br><span class="line">    public JasonPartitioner(VerifiableProperties verifiableProperties) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public int partition(Object key, int numPartitions) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            int partitionNum = Integer.parseInt((String) key);</span><br><span class="line">            return Math.abs(Integer.parseInt((String) key) % numPartitions);</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            return Math.abs(key.hashCode() % numPartitions);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果将上例中的类作为partition.class，并通过如下代码发送20条消息（key分别为0，1，2，3）至topic3（包含4个Partition）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public void sendMessage() throws InterruptedException&#123;</span><br><span class="line">　　for(int i = 1; i &lt;= 5; i++)&#123;</span><br><span class="line">　　      List messageList = new ArrayList&lt;KeyedMessage&lt;String, String&gt;&gt;();</span><br><span class="line">　　      for(int j = 0; j &lt; 4; j++）&#123;</span><br><span class="line">　　          messageList.add(new KeyedMessage&lt;String, String&gt;(&quot;topic2&quot;, j+&quot;&quot;, &quot;The &quot; + i + &quot; message for key &quot; + j));</span><br><span class="line">　　      &#125;</span><br><span class="line">　　      producer.send(messageList);</span><br><span class="line">    &#125;</span><br><span class="line">　　producer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>则key相同的消息会被发送并存储到同一个partition里，而且key的序号正好和Partition序号相同。（Partition序号从0开始，本例中的key也从0开始）。下图所示是通过Java程序调用Consumer后打印出的消息列表<br><img src="http://cdn1.infoqstatic.com/statics_s2_20170530-0600u1/resource/articles/kafka-analysis-part-1/zh/resources/0310024.png"></p>
<p><code>发布消息模式</code><br>发布消息通常有两种模式：队列模式（queuing）和发布-订阅模式(publish-subscribe)。队列模式中，consumers可以同时从服务端读取消息，每个消息只被其中一个consumer读到；发布-订阅模式中消息被广播到所有的consumer中。</p>
<h2 id="consumer"><a href="#consumer" class="headerlink" title="consumer"></a>consumer</h2><p><code>Consumers</code><br>Consumers label themselves with a consumer group name, and each record published to a topic is delivered to one consumer instance within each subscribing consumer group. Consumer instances can be in separate processes or on separate machines.<br><img src="http://kafka.apache.org/0102/images/consumer-groups.png"></p>
<p>如上图，两台服务器集群包含四个partition，服务于2个ConsumerGroup</p>
<p><code>Consumer Group</code><br>Consumers可以加入一个consumer组，共同竞争一个topic，topic中的消息将被分发到组中的一个成员中。同一组中的consumer可以在不同的程序中，也可以在不同的机器上。<br>每个topic都有若干数量的consumer组，每个组都是一个逻辑上的“订阅者”，为了容错和更好的稳定性，每个组由若干consumer组成。这其实就是一个发布-订阅模式，只不过订阅者是个组而不是单个consumer。</p>
<p><code>Consumer Group</code>：<br>使用Consumer high level API时，同一Topic的一条消息只能被同一个Consumer Group内的一个Consumer消费，但多个Consumer Group可同时消费这一消息。<br><img src="http://cdn1.infoqstatic.com/statics_s1_20170516-0317u5/resource/articles/kafka-analysis-part-1/zh/resources/0310025.png"></p>
<p>这是Kafka用来实现一个Topic消息的广播（发给所有的Consumer）和单播（发给某一个Consumer）的手段。</p>
<p>一个Topic可以对应多个Consumer Group。如果需要实现广播，只要每个Consumer有一个独立的Group就可以了。要实现单播只要所有的Consumer在同一个Group里。用Consumer Group还可以将Consumer进行自由的分组而不需要多次发送消息到不同的Topic。</p>
<p><code>不同类型消息处理系统划分为不同Consumer Group</code><br>实际上，Kafka的设计理念之一就是同时提供离线处理和实时处理。根据这一特性，可以使用Storm这种实时流处理系统对消息进行实时在线处理，同时使用Hadoop这种批处理系统进行离线处理，还可以同时将数据实时备份到另一个数据中心，只需要保证这三个操作所使用的Consumer属于不同的Consumer Group即可。下图是Kafka在Linkedin的一种简化部署示意图。<br><img src="http://cdn1.infoqstatic.com/statics_s1_20170516-0317u5/resource/articles/kafka-analysis-part-1/zh/resources/0310026.png"></p>
<h2 id="retention-amp-reconsume"><a href="#retention-amp-reconsume" class="headerlink" title="retention &amp; reconsume"></a>retention &amp; reconsume</h2><p><code>ReConsume</code>：<br>The Kafka cluster retains all published records—whether or not they have been consumed—using a configurable retention period. For example, if the retention policy is set to two days, then for the two days after a record is published, it is available for consumption, after which it will be discarded to free up space. </p>
<p><code>retention</code><br>对于传统的message queue而言，一般会删除已经被消费的消息，而Kafka集群会保留所有的消息，无论其被消费与否。</p>
<p>当然，因为磁盘限制，不可能永久保留所有数据（实际上也没必要），因此Kafka提供两种策略删除旧数据。一是基于时间，二是基于Partition文件大小。</p>
<p>例如可以通过配置$KAFKA_HOME&#x2F;config&#x2F;server.properties，让Kafka删除一周前的数据，也可在Partition文件超过1GB时删除旧数据，配置如下所示。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># The minimum age of a log file to be eligible for deletion</span><br><span class="line">log.retention.hours=168</span><br><span class="line"># The maximum size of a log segment file. When this size is reached a new log segment will be created.</span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line"># The interval at which log segments are checked to see if they can be deleted according to the retention policies</span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line"># If log.cleaner.enable=true is set the cleaner will be enabled and individual logs can then be marked for log compaction.</span><br><span class="line">log.cleaner.enable=false</span><br></pre></td></tr></table></figure>

<p>这里要注意，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高Kafka性能无关。选择怎样的删除策略只与磁盘以及具体的需求有关。</p>
<p><code>ReConsume</code><br>另外，Kafka会为每一个Consumer Group保留一些metadata信息——当前消费的消息的position，也即offset。这个offset由Consumer控制。正常情况下Consumer会在消费完一条消息后递增该offset。当然，Consumer也可将offset设成一个较小的值，重新消费一些消息。</p>
<p>因为offet由Consumer控制，所以Kafka broker是无状态的，它不需要标记哪些消息被哪些消费过，也不需要通过broker去保证同一个Consumer Group只有一个Consumer能消费某一条消息，因此也就不需要锁机制，这也为Kafka的高吞吐率提供了有力保障。</p>
<h2 id="leader-amp-follower"><a href="#leader-amp-follower" class="headerlink" title="leader &amp; follower"></a>leader &amp; follower</h2><p><code>broker</code><br>Kafka以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个broker.</p>
<p><code>分布式</code><br>每个分区在Kafka集群的若干服务中都有副本，这样这些持有副本的服务可以共同处理数据和请求，副本数量是可以配置的。副本使Kafka具备了容错能力。</p>
<p><code>Leader &amp; Followers</code>：<br>Each partition has one server which acts as the “leader” and zero or more servers which act as “followers”. The leader handles all read and write requests for the partition while the followers passively replicate the leader. If the leader fails, one of the followers will automatically become the new leader. Each server acts as a leader for some of its partitions and a follower for others so load is well balanced within the cluster.</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p><code>消息持久化</code><br>（1）支持消息持久化：Broker没有副本备份，但kafka将消息进行持久化操作，以免丢失。当Consumer到kafka的broker中获取数据时，broker不会直接给consumer消费，而是把数据先保存到broker本地日志文件中（具体路径可配）。另外日志的添加采用追加方式进行持久化，达到一个消息有序持久化效果。写入日志文件：缓存到一定阈值之后，再读写磁盘进行IO操作，提高性能</p>
<p><code>as a storage system</code><br>Data written to Kafka is written to disk and replicated for fault-tolerance. Kafka allows producers to wait on acknowledgement so that a write isn’t considered complete until it is fully replicated and guaranteed to persist even if the server written to fails.</p>
<p>As a result of taking storage seriously and allowing the clients to control their read position, you can think of Kafka as a kind of special purpose distributed filesystem dedicated to high-performance, low-latency commit log storage, replication, and propagation.</p>
<p><code>as a stream processing</code><br>In Kafka a stream processor is anything that takes continual streams of data from input topics, performs some processing on this input, and produces continual streams of data to output topics.</p>
<p>It is possible to do simple processing directly using the producer and consumer APIs. However for more complex transformations Kafka provides a fully integrated Streams API. This allows building applications that do non-trivial processing that compute aggregations off of streams or join streams together.</p>
<p><code>故障快速定位</code><br>（2）谁消费了message，由消费者确定和维护这类信息（具体有zookeeper记录维护），broker不保存。<br>（3）消费时发生故障，kafka可快速定位到故障时没消费的那条数据。 consumer如何确定哪些消息是它没有消费的？zk来记录哪条消息消费情况，如何快速的找到没有消费的消息就涉及到kafka的稀疏索引机制。</p>
<p><code>Push vs. Pull</code>：<br>作为一个消息系统，Kafka遵循了传统的方式，选择由Producer向broker push消息并由Consumer从broker pull消息。一些logging-centric system，比如Facebook的Scribe和Cloudera的Flume，采用push模式。事实上，push模式和pull模式各有优劣。</p>
<p>push模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。push模式的目标是尽可能以最快速度传递消息，但是这样很容易造成Consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据Consumer的消费能力以适当的速率消费消息。</p>
<p>对于Kafka而言，pull模式更合适。pull模式可简化broker的设计，Consumer可自主控制消费消息的速率，同时Consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。</p>
<p><code>Kafka delivery guarantee</code>：<br>有这么几种可能的delivery guarantee：</p>
<ul>
<li>At most once 消息可能会丢，但绝不会重复传输</li>
<li>At least one 消息绝不会丢，但可能会重复传输</li>
<li>Exactly once 每条消息肯定会被传输一次且仅传输一次，很多时候这是用户所想要的。</li>
</ul>
<p>当Producer向broker发送消息时，一旦这条消息被commit，因数replication的存在，它就不会丢。但是如果Producer发送数据给broker后，遇到网络问题而造成通信中断，那Producer就无法判断该条消息是否已经commit。虽然Kafka无法确定网络故障期间发生了什么，但是Producer可以生成一种类似于主键的东西，发生故障时幂等性的重试多次，这样就做到了Exactly once。</p>
<p>接下来讨论的是消息从broker到Consumer的delivery guarantee语义。（仅针对Kafka consumer high level API）。Consumer在从broker读取消息后，可以选择commit，该操作会在Zookeeper中保存该Consumer在该Partition中读取的消息的offset。该Consumer下一次再读该Partition时会从下一条开始读取。如未commit，下一次读取的开始位置会跟上一次commit之后的开始位置相同。当然可以将Consumer设置为autocommit，即Consumer一旦读到数据立即自动commit。</p>
<p>如果只讨论这一读取消息的过程，那Kafka是确保了Exactly once。但实际使用中应用程序并非在Consumer读取完数据就结束了，而是要进行进一步处理，而数据处理与commit的顺序在很大程度上决定了消息从broker和consumer的delivery guarantee semantic：</p>
<ul>
<li>读完消息先commit再处理消息。这种模式下，如果Consumer在commit后还没来得及处理消息就crash了，下次重新开始工作后就无法读到刚刚已提交而未处理的消息，这就对应于At most once</li>
<li>读完消息先处理再commit。这种模式下，如果在处理完消息之后commit之前Consumer crash了，下次重新开始工作时还会处理刚刚未commit的消息，实际上该消息已经被处理过了。这就对应于At least once。</li>
</ul>
<hr>
<h1 id="QuickStart"><a href="#QuickStart" class="headerlink" title="QuickStart"></a>QuickStart</h1><ul>
<li>Download、tar -xvf、</li>
</ul>
<h2 id="start-zookeeper"><a href="#start-zookeeper" class="headerlink" title="start zookeeper"></a>start zookeeper</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/zookeeper-server-start.sh config/zookeeper.properties &amp;</span><br></pre></td></tr></table></figure>

<p>2181端口</p>
<h2 id="start-kafka-server"><a href="#start-kafka-server" class="headerlink" title="start kafka server"></a>start kafka server</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-server-start.sh config/server.properties &amp;</span><br></pre></td></tr></table></figure>

<p>9092端口</p>
<h2 id="create-a-topic"><a href="#create-a-topic" class="headerlink" title="create a topic"></a>create a topic</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test</span><br></pre></td></tr></table></figure>

<h2 id="show-all-topics"><a href="#show-all-topics" class="headerlink" title="show all topics"></a>show all topics</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br></pre></td></tr></table></figure>

<h2 id="send-messages"><a href="#send-messages" class="headerlink" title="send messages"></a>send messages</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</span><br></pre></td></tr></table></figure>

<p>then type messages </p>
<h2 id="start-a-consumer"><a href="#start-a-consumer" class="headerlink" title="start a consumer"></a>start a consumer</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning</span><br></pre></td></tr></table></figure>

<p>then you can receive messages</p>
<h2 id="setting-up-a-multi-broker-cluster"><a href="#setting-up-a-multi-broker-cluster" class="headerlink" title="setting up a multi-broker cluster"></a>setting up a multi-broker cluster</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cp config/server.properties config/server-1.properties</span><br><span class="line">cp config/server.properties config/server-2.properties</span><br><span class="line"></span><br><span class="line">edit new files：</span><br><span class="line">config/server-1.properties:</span><br><span class="line">    broker.id=1</span><br><span class="line">    listeners=PLAINTEXT://:9093</span><br><span class="line">    log.dir=/tmp/kafka-logs-1</span><br><span class="line"></span><br><span class="line">config/server-2.properties:</span><br><span class="line">    broker.id=2</span><br><span class="line">    listeners=PLAINTEXT://:9094</span><br><span class="line">    log.dir=/tmp/kafka-logs-2</span><br></pre></td></tr></table></figure>

<h2 id="start-two-new-brokers："><a href="#start-two-new-brokers：" class="headerlink" title="start two new brokers："></a>start two new brokers：</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-server-start.sh config/server-1.properties &amp;</span><br><span class="line">...</span><br><span class="line">./bin/kafka-server-start.sh config/server-2.properties &amp;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h2 id="create-a-new-topic-with-three-replications"><a href="#create-a-new-topic-with-three-replications" class="headerlink" title="create a new topic with three replications"></a>create a new topic with three replications</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic</span><br></pre></td></tr></table></figure>

<h2 id="see-info-of-this-topic"><a href="#see-info-of-this-topic" class="headerlink" title="see info of this topic"></a>see info of this topic</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic</span><br></pre></td></tr></table></figure>

<h2 id="kill-one-broker"><a href="#kill-one-broker" class="headerlink" title="kill one broker"></a>kill one broker</h2><p><code>ps aux | grep server-1.properties</code><br>checkout the pid<br><code>kill -9 pid</code></p>
<h2 id="use-kafka-connector-to-import-x2F-export-data"><a href="#use-kafka-connector-to-import-x2F-export-data" class="headerlink" title="use kafka connector to import&#x2F;export data"></a>use kafka connector to import&#x2F;export data</h2><p>Kafka Connect is a tool included with Kafka that imports and exports data to Kafka<br>first create data file which to be imported to kafka broker<br><code>echo -e &quot;foo\nbar&quot; &gt; test.txt</code></p>
<p>start two connectors running in standalone mode, which means they run in a single, local, dedicated process<br>source connector import data to kafka，sink connector export data in kafka to file </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties</span><br></pre></td></tr></table></figure>

<h2 id="use-kafka-streams-to-process-data"><a href="#use-kafka-streams-to-process-data" class="headerlink" title="use kafka streams to process data"></a>use kafka streams to process data</h2><p>Kafka Streams is a client library of Kafka for real-time stream processing and analyzing data stored in Kafka brokers</p>
<p>this demo implements the WordCount algorithm</p>
<p>first compile the src，then put the class into the bin dir</p>
<p>make the input file：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo -e &quot;all streams lead to kafka\nhello kafka streams\njoin kafka summit&quot; &gt; file-input.txt</span><br></pre></td></tr></table></figure>

<p>create the topic for stream test：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-topics.sh --create \</span><br><span class="line">            --zookeeper localhost:2181 \</span><br><span class="line">            --replication-factor 1 \</span><br><span class="line">            --partitions 1 \</span><br><span class="line">            --topic streams-file-input</span><br></pre></td></tr></table></figure>

<p>input the file to the topic：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streams-file-input &lt; file-input.txt</span><br></pre></td></tr></table></figure>

<p>run kafka stream process：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo</span><br></pre></td></tr></table></figure>

<p>this stream process read input from topic and process，then output the result into topic defined in src</p>
<p>view result from topic defined in src：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 \</span><br><span class="line">            --topic streams-wordcount-output \</span><br><span class="line">            --from-beginning \</span><br><span class="line">            --formatter kafka.tools.DefaultMessageFormatter \</span><br><span class="line">            --property print.key=true \</span><br><span class="line">            --property print.value=true \</span><br><span class="line">            --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \</span><br><span class="line">            --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer</span><br></pre></td></tr></table></figure>

<p>what Kafka Streams is doing here is to leverage the duality between a table and a changelog stream (here: table &#x3D; the KTable, changelog stream &#x3D; the downstream KStream)</p>
<p>you can publish every change of the table to a stream, and if you consume the entire changelog stream from beginning to end, you can reconstruct the contents of the table&#96;</p>
<h1 id="UseCases"><a href="#UseCases" class="headerlink" title="UseCases"></a>UseCases</h1><blockquote>
<p>介绍Kafka的应用场景</p>
</blockquote>
<ul>
<li>Messaging<br>Message brokers are used for a variety of reasons (to <code>decouple processing from data producers, to buffer unprocessed messages</code>, etc).</li>
</ul>
<p>In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.</p>
<ul>
<li>Website Activity Tracking<br>This means <code>site activity (page views, searches, or other actions users may take) is published to central topics with one topic per activity type</code>.</li>
</ul>
<p>These feeds are available for subscription for a range of use cases including real-time processing, real-time monitoring, and loading into Hadoop or offline data warehousing systems for offline processing and reporting.</p>
<ul>
<li><p>Metrics<br>aggregating statistics from distributed applications to produce centralized feeds of operational data</p>
</li>
<li><p>Log Aggregation<br>Log aggregation typically <code>collects physical log files off servers and puts them in a central place</code> (a file server or HDFS perhaps) for processing.</p>
</li>
</ul>
<p>Kafka abstracts away the details of files and gives a cleaner abstraction of log or event data as a stream of messages. This allows for lower-latency processing and easier support for multiple data sources and distributed data consumption.</p>
<ul>
<li>Stream Processing<br>Many users of Kafka process data in processing pipelines consisting of multiple stages, where <code>raw input data is consumed from Kafka topics and then aggregated, enriched, or otherwise transformed into new topics for further consumption or follow-up processing</code></li>
</ul>
<p>For example, a processing pipeline for recommending news articles might crawl article content from RSS feeds and publish it to an “articles” topic; further processing might normalize or deduplicate this content and published the cleansed article content to a new topic; a final processing stage might attempt to recommend this content to users. </p>
<p>Starting in 0.10.0.0, a light-weight but powerful stream processing library called Kafka Streams is available in Apache Kafka to perform such data processing as described above. </p>
<ul>
<li>Event Sourcing<br>Event sourcing is a style of application design where <code>state changes are logged as a time-ordered sequence of records</code>.</li>
</ul>
<p>Kafka’s <code>support for very large stored log data</code> makes it an excellent backend for an application built in this style.</p>
<ul>
<li>Commit log<br>Kafka can serve as a kind of external commit-log for a distributed system.</li>
</ul>
<p>The log helps <code>replicate data between nodes and acts as a re-syncing mechanism for failed nodes to restore their data</code>.</p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>西米大人
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://dcbupt.github.io/2017/05/22/blog_article/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/Kafka/" title="Kafka">http://dcbupt.github.io/2017/05/22/blog_article/好文转载/Kafka/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        

<blockquote class="blockquote-center" style="color: #f0ad4e">完 ♥ 结</blockquote>



        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2017/05/21/blog_article/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/SSL%E5%8D%8F%E8%AE%AE%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6%E6%A6%82%E8%BF%B0/" rel="prev" title="SSL协议运行机制概述">
      <i class="fa fa-chevron-left"></i> SSL协议运行机制概述
    </a></div>
      <div class="post-nav-item">
    <a href="/2017/05/26/blog_article/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/gradle/" rel="next" title="gradle">
      gradle <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%A5%E9%97%A8"><span class="nav-number">1.</span> <span class="nav-text">入门</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#topic-amp-partition"><span class="nav-number">1.1.</span> <span class="nav-text">topic &amp; partition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#producer"><span class="nav-number">1.2.</span> <span class="nav-text">producer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#consumer"><span class="nav-number">1.3.</span> <span class="nav-text">consumer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#retention-amp-reconsume"><span class="nav-number">1.4.</span> <span class="nav-text">retention &amp; reconsume</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#leader-amp-follower"><span class="nav-number">1.5.</span> <span class="nav-text">leader &amp; follower</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96"><span class="nav-number">1.6.</span> <span class="nav-text">其他</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#QuickStart"><span class="nav-number">2.</span> <span class="nav-text">QuickStart</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#start-zookeeper"><span class="nav-number">2.1.</span> <span class="nav-text">start zookeeper</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#start-kafka-server"><span class="nav-number">2.2.</span> <span class="nav-text">start kafka server</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#create-a-topic"><span class="nav-number">2.3.</span> <span class="nav-text">create a topic</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#show-all-topics"><span class="nav-number">2.4.</span> <span class="nav-text">show all topics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#send-messages"><span class="nav-number">2.5.</span> <span class="nav-text">send messages</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#start-a-consumer"><span class="nav-number">2.6.</span> <span class="nav-text">start a consumer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#setting-up-a-multi-broker-cluster"><span class="nav-number">2.7.</span> <span class="nav-text">setting up a multi-broker cluster</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#start-two-new-brokers%EF%BC%9A"><span class="nav-number">2.8.</span> <span class="nav-text">start two new brokers：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#create-a-new-topic-with-three-replications"><span class="nav-number">2.9.</span> <span class="nav-text">create a new topic with three replications</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#see-info-of-this-topic"><span class="nav-number">2.10.</span> <span class="nav-text">see info of this topic</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kill-one-broker"><span class="nav-number">2.11.</span> <span class="nav-text">kill one broker</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#use-kafka-connector-to-import-x2F-export-data"><span class="nav-number">2.12.</span> <span class="nav-text">use kafka connector to import&#x2F;export data</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#use-kafka-streams-to-process-data"><span class="nav-number">2.13.</span> <span class="nav-text">use kafka streams to process data</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#UseCases"><span class="nav-number">3.</span> <span class="nav-text">UseCases</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="西米大人"
      src="/images/me.png">
  <p class="site-author-name" itemprop="name">西米大人</p>
  <div class="site-description" itemprop="description"><blockquote class="blockquote-center">优秀的人，不是不合群，而是他们合群的人里面没有你</blockquote></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">151</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/dcbupt" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;dcbupt" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:byrddc@hotmail.com" title="E-Mail → mailto:byrddc@hotmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">西米大人</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">388k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">10:47</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              leancloudSelector(url).innerText = 'Counter not initialized! More info at console err msg.';
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"OBdFsOOtfIDcNnWdzkGbICmj-gzGzoHsz","app_key":"89LmL4tP7hkyChAmEleq2MdO","server_url":null,"security":true};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



<script src="/js/love.js"></script>

<script src="/js/particle.js"></script>

  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('https://cdn.jsdelivr.net/npm/valine@1.4.18/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'OBdFsOOtfIDcNnWdzkGbICmj-gzGzoHsz',
      appKey     : '89LmL4tP7hkyChAmEleq2MdO',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
